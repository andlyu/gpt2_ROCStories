{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7274a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When converting to Python file, change display function to print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c10378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c5a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DATASET_TO_USE = \"UNION\"\n",
    "LOSS_FN = \"cross_entorpy\"\n",
    "#43 for log logloss\n",
    "#43.6 for cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ea1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if(DATASET_TO_USE != 'UNION'):\n",
    "    data = pd.read_csv('../Dataset/0OYkPK', sep=\",\", header=None)\n",
    "    data.columns = data.iloc[0]\n",
    "    data = data[1:]\n",
    "    data['full'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']+ \" \" + data['sentence5']\n",
    "    data['input'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']\n",
    "\n",
    "    val_data = pd.read_csv('../Dataset/XWjas1', sep=\",\", header=None)\n",
    "    val_data.columns = val_data.iloc[0]\n",
    "    val_data = val_data[1:]\n",
    "    #val_data['InputSentence5'] = val_data['RandomFifthSentenceQuiz1']\n",
    "    #val_data['InputSentence5'] = val_data['RandomFifthSentenceQuiz1']\n",
    "    val_data['InputSentence5']  = np.where(val_data['AnswerRightEnding']== '1', val_data['RandomFifthSentenceQuiz1'], val_data['RandomFifthSentenceQuiz2'])\n",
    "    val_data['full'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']+ \" \" + val_data['InputSentence5']\n",
    "    val_data['input'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']\n",
    "\n",
    "    print('The shapes of data and val_data')\n",
    "    print(data.shape)\n",
    "    print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d22ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes of data and val_data\n",
      "(88344, 7)\n",
      "(4908, 7)\n"
     ]
    }
   ],
   "source": [
    "if(DATASET_TO_USE == 'UNION'):\n",
    "    text_file = open('../train_data/train_human.txt', \"r\") #Read UNION File\n",
    "    lines = text_file.readlines()\n",
    "    lines = [item[:-1]for item in lines]\n",
    "    text_file.close()\n",
    "\n",
    "    data = pd.DataFrame(np.reshape(lines,(-1,6))) #Convert to pandas format\n",
    "    data = data[[0,1,2,3,4]]\n",
    "    data.columns = ['sentence1','sentence2','sentence3','sentence4','sentence5']\n",
    "    data['full'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']+ \" \" + data['sentence5']\n",
    "    data['input'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']\n",
    "    #data\n",
    "    \n",
    "\n",
    "    text_file = open('../train_data/dev_human.txt', \"r\") #Read UNION File\n",
    "    lines = text_file.readlines()\n",
    "    lines = [item[:-1]for item in lines]\n",
    "    text_file.close()\n",
    "\n",
    "    val_data = pd.DataFrame(np.reshape(lines,(-1,6))) #Convert to pandas format\n",
    "    val_data = val_data[[0,1,2,3,4]]\n",
    "    val_data.columns = ['InputSentence1','InputSentence2','InputSentence3','InputSentence4','InputSentence5']\n",
    "    val_data['full'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']+ \" \" + val_data['InputSentence5']\n",
    "    val_data['input'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']\n",
    "    val_data.full\n",
    "    \n",
    "    print('The shapes of data and val_data')\n",
    "    print(data.shape)\n",
    "    print(val_data.shape)\n",
    "else:\n",
    "    print('UNION was not loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c77ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device('cuda:1'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_sentences = [x for x in data.full]\n",
    "\n",
    "val_sentences = [x for x in val_data.input]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029965b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len  10000\n",
      "val_len  4908\n"
     ]
    }
   ],
   "source": [
    "all_sentences[0:10]\n",
    "print('train_len ', len(all_sentences))\n",
    "print('val_len ', len(val_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca1316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "#get pretrained tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<sos>', pad_token='<pad>', padding_side = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56db8927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 117\n",
      "max_val 101\n"
     ]
    }
   ],
   "source": [
    "max_len = int(np.max([len(tokenizer.encode(s)) for s in all_sentences]))\n",
    "print(f\"max_len {max_len}\")\n",
    "\n",
    "max_val = int(np.max([len(tokenizer.encode(s)) for s in val_sentences]))\n",
    "print(f\"max_val {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0cdbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_seq(sent,tokenizer,max_length):\n",
    "    return tokenizer('<sos>'+ sent , truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "class ROCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, gpt2_type=\"gpt2\", max_length=max_len):\n",
    "\n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for sentence in sentences:      \n",
    "            encodings = tokenize_seq(sentence,tokenizer,max_length)\n",
    "            #encodings['input_ids'] = [x if x != 50258 else -100 for x in encodings['input_ids']]\n",
    "            #print(encodings['input_ids'])\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]   \n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b2b6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd0d6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of Dataset\n",
    "train_set = ROCDataset(all_sentences, tokenizer, max_length=max_len)\n",
    "val_set = ROCDataset(val_sentences, tokenizer, max_length=max_val)\n",
    "\n",
    "\n",
    "#train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "#print(\"train_size :\",train_size)\n",
    "#print(\"val_size   :\",val_size)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de1844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50257,    58,    44, 21358,\n",
      "           60,   705,    82,  3397,   547, 22652,   764,   685,    44, 21358,\n",
      "           60,   373, 22652,   355,   880,   764,   262,  7519,  1297,   465,\n",
      "         3397,   340,   373, 27942,   764,   465,  3397,  7247,   290,  3066,\n",
      "          284,   787,   257,  1487,   764,   484,  1392,  2405,   290,   685,\n",
      "           44, 21358,    60,   319,   257,  5496,   764]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e8d8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataloaders\n",
    "train_dataloader = DataLoader(train_set,  sampler = RandomSampler(train_set), batch_size = BATCH_SIZE)\n",
    "validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "898fedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default config\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# Load pretrained gpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create device\n",
    "device = torch.device(\"cuda:1\")\n",
    "model.to(device)\n",
    "\n",
    "lr = 5e-5\n",
    "if(LOSS_FN == 'perplexity'):\n",
    "    lr = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4022e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "#call model with a batch of input\n",
    "def process_one_batch(batch):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_labels = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    b_labels[b_labels == 50258] = -100\n",
    "    outputs  = model(b_input_ids,  attention_mask = b_masks,labels=b_labels)\n",
    "    return outputs\n",
    "\n",
    "#call model with a batch of input\n",
    "def output_one_batch(batch):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_labels = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    b_labels[b_labels == 50258] = -100\n",
    "    outputs  = model(b_input_ids,  num_beams=1 ,  attention_mask = b_masks,labels=b_labels)\n",
    "    return outputs\n",
    "\n",
    "#do one epoch for training\n",
    "def train_epoch():\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = process_one_batch(batch)\n",
    "        loss = outputs[0]  \n",
    "        if(LOSS_FN == 'perplexity'):\n",
    "            loss = torch.log(loss/BATCH_SIZE)\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
    "    print(\"avg_train_loss\",avg_train_loss)  \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 training epoch : \",elapsed_time)\n",
    "    return avg_train_loss\n",
    "\n",
    "#do one epoch for eval\n",
    "def eval_epoch():\n",
    "    t0 = time.time()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:            \n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = process_one_batch(batch)\n",
    "            loss = outputs[0]\n",
    "            if(LOSS_FN == 'perplexity'):\n",
    "                loss = torch.log(loss/BATCH_SIZE)\n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss         \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"avg_val_loss\",avg_val_loss) \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 eval epoch : \",elapsed_time)\n",
    "    return avg_val_loss\n",
    "    \n",
    "\n",
    "#Runs the model on a set number of batches and saves the results to a json file\n",
    "def save_results( num_batches = 15, iter = 0):\n",
    "    indexes_list = []\n",
    "    inputs_list = []\n",
    "    predicted_list = []\n",
    "    expected_list = []\n",
    "    \n",
    "    model.save_pretrained(\"saved_model_temp\")\n",
    "\n",
    "    #for i in tqdm(range(num_examples)):\n",
    "    for i, batch in enumerate(tqdm(validation_dataloader)):\n",
    "        if(num_batches != None and i>num_batches):\n",
    "            break\n",
    "        # Story is:\n",
    "        #input_ids = tokenizer(val_data.input.iloc[i], return_tensors='pt')\n",
    "        #input_ids.to(device)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        \n",
    "        greedy_output = model.generate(\n",
    "                b_input_ids,  #check stars   \n",
    "                num_beams=1 ,\n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True, \n",
    "                max_length=150,\n",
    "                tempterature = .9,\n",
    "                top_p = .7\n",
    "                )\n",
    "        \n",
    "        print(greedy_output['sequences'].shape)\n",
    "        output = tokenizer.batch_decode(greedy_output['sequences'])\n",
    "        len_input = len(val_data.input.iloc[i])\n",
    "        #output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "        b_outputs = []\n",
    "        \n",
    "        for b_idx in range(BATCH_SIZE):\n",
    "            if(len(output) <= b_idx):\n",
    "                break\n",
    "            idx = BATCH_SIZE * i + b_idx\n",
    "            end_words = val_data.InputSentence4\n",
    "            in_words = val_data.input.iloc[idx]\n",
    "            \n",
    "            indexes_list.append(idx)\n",
    "            inputs_list.append(val_data.input.iloc[idx])\n",
    "            #predicted_list.append(output[b_idx])\n",
    "            if(len(re.split('\\? |! |\\. ',output[b_idx]))<5):\n",
    "                predicted_list.append('bad gen')\n",
    "            else:\n",
    "                predicted_list.append(re.split('\\? |! |\\. ',output[b_idx])[4] + '.')\n",
    "            expected_list.append(val_data.InputSentence5.iloc[idx])\n",
    "            \n",
    "    outputs = pd.DataFrame()\n",
    "    outputs['inputs'] = inputs_list\n",
    "    outputs['predicted'] = predicted_list\n",
    "    outputs['expected'] = expected_list\n",
    "    \n",
    "    print(outputs[:5])\n",
    "    \n",
    "    data = {}\n",
    "    data['ex'] = []\n",
    "    for i in range(len(indexes_list)):\n",
    "        data['ex'].append({\n",
    "            'idx': indexes_list[i],\n",
    "            'input': inputs_list[i],\n",
    "            'prediction': predicted_list[i],\n",
    "            'expected': expected_list[i]\n",
    "\n",
    "        })\n",
    "\n",
    "    with open('test_cases/test_cases'+ str(iter)+'.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fb071ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [05:17<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_train_loss -1.661562880784273\n",
      "elapsed time for 1 training epoch :  0:05:18\n",
      "avg_val_loss -1.8169178492859832\n",
      "elapsed time for 1 eval epoch :  0:00:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/307 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 1/307 [00:00<03:48,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/307 [00:01<03:45,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/307 [00:02<03:43,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/307 [00:02<03:42,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/307 [00:03<03:41,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/307 [00:04<03:41,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n",
      "                                              inputs  \\\n",
      "0  [MALE] is skiing . [MALE] falls off his skis ....   \n",
      "1  [MALE] let his friend borrow his phone . the f...   \n",
      "2  [FEMALE] asked her boyfriend to buy some coat ...   \n",
      "3  [MALE] was driving to work . he got pulled ove...   \n",
      "4  [MALE] 's mailman was n't very competent . whe...   \n",
      "\n",
      "                                           predicted  \\\n",
      "0  [MALE] was a friend [MALE] was a very nervous ...   \n",
      "1  [MALE] was a very good friend of [MALE] was a ...   \n",
      "2    he was a friend of [MALE] was a friend of mine.   \n",
      "3  [MALE] was a little boy [MALE] was a little bo...   \n",
      "4  he was very nervous about the weather was comi...   \n",
      "\n",
      "                                            expected  \n",
      "0                 [MALE] has broken his leg skiing .  \n",
      "1                     the phone died shortly after .  \n",
      "2        she had to go back and get the right kind .  \n",
      "3   [MALE] agreed to fix it and only got a warning .  \n",
      "4  the new mailman was happy to have [MALE] as a ...  \n",
      "saving loss figure to losses.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 239/625 [02:00<03:14,  1.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b2a0da1e6c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-530df07f0e92>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for i in range(4):\n",
    "    train_loss.append(train_epoch())\n",
    "    val_loss.append(eval_epoch())\n",
    "    save_results(5, i)\n",
    "    \n",
    "    \n",
    "    print('saving loss figure to losses.png')\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.show()\n",
    "    plt.savefig('losses.png')\n",
    "    model.save_pretrained(\"UNION_model\")\n",
    "\n",
    "\n",
    "\n",
    "save_results(None, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42137f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_results(None, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c65b7",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.makedirs(\"saved_model\")\n",
    "#model.save_pretrained(\"saved_model\")\n",
    "#model = .from_pretrained(\"path/to/awesome-name-you-picked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default config\n",
    "#configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "# Load pretrained gpt2\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"saved_model/\", config=configuration)\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"saved_model/\")#, config=configuration)\n",
    "\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # Create device\n",
    "# device = torch.device(\"cuda:1\")\n",
    "# model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr = 0.0000005)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d68294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#do one epoch for eval\n",
    "# def save_results( num_batches = 15, iter = 0):\n",
    "#     indexes_list = []\n",
    "#     inputs_list = []\n",
    "#     predicted_list = []\n",
    "#     expected_list = []\n",
    "        \n",
    "#     #for i in tqdm(range(num_examples)):\n",
    "#     for i, batch in enumerate(tqdm(validation_dataloader)):\n",
    "#         if(num_batches != None and i>num_batches):\n",
    "#             break\n",
    "#         # Story is:\n",
    "#         #input_ids = tokenizer(val_data.input.iloc[i], return_tensors='pt')\n",
    "#         #input_ids.to(device)\n",
    "#         b_input_ids = batch[0].to(device)\n",
    "        \n",
    "#         greedy_output = model.generate(\n",
    "#                 b_input_ids,  #check stars   \n",
    "#                 num_beams=2 ,\n",
    "#                 return_dict_in_generate=True, \n",
    "#                 output_scores=True, \n",
    "#                 max_length=150,\n",
    "#                 #tempterature = 5,\n",
    "#                 top_p = 10\n",
    "#                 )\n",
    "        \n",
    "#         print(greedy_output['sequences'].shape)\n",
    "#         output = tokenizer.batch_decode(greedy_output['sequences'])\n",
    "#         len_input = len(val_data.input.iloc[i])\n",
    "#         #output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "#         b_outputs = []\n",
    "        \n",
    "#         for b_idx in range(BATCH_SIZE):\n",
    "#             if(len(output) <= b_idx):\n",
    "#                 break\n",
    "#             idx = BATCH_SIZE * i + b_idx\n",
    "#             end_words = val_data.InputSentence4\n",
    "#             in_words = val_data.input.iloc[idx]\n",
    "#             #if(len(output) <= b_idx):\n",
    "#             #    break\n",
    "#             #b_out = output[b_idx]\n",
    "#             #pred_sent = b_out[b_out.index('<sos>') + len(in_words) + 13:]\n",
    "#             #if(pred_sent.find('.') != -1):\n",
    "#             #    pred_sent = pred_sent[:pred_sent.index('.')+1]\n",
    "#             #else:\n",
    "#             #    pred_sent = pred_sent\n",
    "            \n",
    "#             indexes_list.append(idx)\n",
    "#             inputs_list.append(val_data.input.iloc[idx])\n",
    "#             #predicted_list.append(output[b_idx])\n",
    "#             if(len(re.split('\\? |! |\\. ',output[b_idx]))<5):\n",
    "#                 predicted_list.append('bad gen')\n",
    "#             else:\n",
    "#                 predicted_list.append(re.split('\\? |! |\\. ',output[b_idx])[4] + '.')\n",
    "#             expected_list.append(val_data.InputSentence5.iloc[idx])\n",
    "            \n",
    "#     outputs = pd.DataFrame()\n",
    "#     outputs['inputs'] = inputs_list\n",
    "#     outputs['predicted'] = predicted_list\n",
    "#     outputs['expected'] = expected_list\n",
    "    \n",
    "#     print(outputs[:5])\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "#     data = {}\n",
    "#     data['ex'] = []\n",
    "#     for i in range(len(indexes_list)):\n",
    "#         data['ex'].append({\n",
    "#             'idx': indexes_list[i],\n",
    "#             'input': inputs_list[i],\n",
    "#             'prediction': predicted_list[i],\n",
    "#             'expected': expected_list[i]\n",
    "\n",
    "#         })\n",
    "\n",
    "#     with open('test_cases/test_cases'+ str(iter)+'.json', 'w') as outfile:\n",
    "#         json.dump(data, outfile)\n",
    "        \n",
    "# save_results(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d33cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt23",
   "language": "python",
   "name": "gpt23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
