{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d65ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When converting to Python file, change display function to print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9de0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb6b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DATASET_TO_USE = \"UNION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5561f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/0OYkPK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-efb2d470ce8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Dataset/0OYkPK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpt23/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset/0OYkPK'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Dataset/0OYkPK', sep=\",\", header=None)\n",
    "data.columns = data.iloc[0]\n",
    "data = data[1:]\n",
    "data['full'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']+ \" \" + data['sentence5']\n",
    "data['input'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']\n",
    "\n",
    "val_data = pd.read_csv('../Dataset/XWjas1', sep=\",\", header=None)\n",
    "val_data.columns = val_data.iloc[0]\n",
    "val_data = val_data[1:]\n",
    "#val_data['InputSentence5'] = val_data['RandomFifthSentenceQuiz1']\n",
    "#val_data['InputSentence5'] = val_data['RandomFifthSentenceQuiz1']\n",
    "val_data['InputSentence5']  = np.where(val_data['AnswerRightEnding']== '1', val_data['RandomFifthSentenceQuiz1'], val_data['RandomFifthSentenceQuiz2'])\n",
    "val_data['full'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']+ \" \" + val_data['InputSentence5']\n",
    "val_data['input'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']\n",
    "\n",
    "print('The shapes of data and val_data')\n",
    "print(data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4693f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../UNION/train_data/train_human.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a3e14c299624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_TO_USE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'UNION'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../UNION/train_data/train_human.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Read UNION File\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../UNION/train_data/train_human.txt'"
     ]
    }
   ],
   "source": [
    "if(DATASET_TO_USE == 'UNION'):\n",
    "    text_file = open('../train_data/train_human.txt', \"r\") #Read UNION File\n",
    "    lines = text_file.readlines()\n",
    "    lines = [item[:-1]for item in lines]\n",
    "    text_file.close()\n",
    "\n",
    "    data = pd.DataFrame(np.reshape(lines,(-1,6))) #Convert to pandas format\n",
    "    data = data[[0,1,2,3,4]]\n",
    "    data.columns = ['sentence1','sentence2','sentence3','sentence4','sentence5']\n",
    "    data['full'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']+ \" \" + data['sentence5']\n",
    "    data['input'] = data['sentence1']+ \" \" + data['sentence2']+ \" \" + data['sentence3']+ \" \" + data['sentence4']\n",
    "    #data\n",
    "\n",
    "    text_file = open('../train_data/dev_human.txt', \"r\") #Read UNION File\n",
    "    lines = text_file.readlines()\n",
    "    lines = [item[:-1]for item in lines]\n",
    "    text_file.close()\n",
    "\n",
    "    val_data = pd.DataFrame(np.reshape(lines,(-1,6))) #Convert to pandas format\n",
    "    val_data = val_data[[0,1,2,3,4]]\n",
    "    val_data.columns = ['InputSentence1','InputSentence2','InputSentence3','InputSentence4','InputSentence5']\n",
    "    val_data['full'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']+ \" \" + val_data['InputSentence5']\n",
    "    val_data['input'] = val_data['InputSentence1']+ \" \" + val_data['InputSentence2']+ \" \" + val_data['InputSentence3']+ \" \" + val_data['InputSentence4']\n",
    "    val_data.full\n",
    "    \n",
    "    print('The shapes of data and val_data')\n",
    "    print(data.shape)\n",
    "    print(val_data.shape)\n",
    "else:\n",
    "    print('UNION was not loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc16c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device('cuda:1'):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "all_sentences = [x for x in data.full[:20000]]\n",
    "\n",
    "val_sentences = [x for x in val_data.input]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bb737",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences[0:10]\n",
    "print('train_len ', len(all_sentences))\n",
    "print('val_len ', len(val_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "#get pretrained tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<sos>', pad_token='<pad>', padding_side = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01895681",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = int(np.max([len(tokenizer.encode(s)) for s in all_sentences]))\n",
    "print(f\"max_len {max_len}\")\n",
    "\n",
    "max_val = int(np.max([len(tokenizer.encode(s)) for s in val_sentences]))\n",
    "print(f\"max_val {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aef3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_seq(sent,tokenizer,max_length):\n",
    "    return tokenizer('<sos>'+ sent , truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "class JapanDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, gpt2_type=\"gpt2\", max_length=max_len):\n",
    "\n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for sentence in sentences:      \n",
    "            encodings = tokenize_seq(sentence,tokenizer,max_length)\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]   \n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of Dataset\n",
    "train_set = JapanDataset(all_sentences, tokenizer, max_length=max_len)\n",
    "val_set = JapanDataset(val_sentences, tokenizer, max_length=max_val)\n",
    "\n",
    "\n",
    "#train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "#print(\"train_size :\",train_size)\n",
    "#print(\"val_size   :\",val_size)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataloaders\n",
    "train_dataloader = DataLoader(train_set,  sampler = RandomSampler(train_set), batch_size = BATCH_SIZE)\n",
    "validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97937352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default config\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# Load pretrained gpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create device\n",
    "device = torch.device(\"cuda:1\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 5e-5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5594ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "#call model with a batch of input\n",
    "def process_one_batch(batch):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_labels = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    outputs  = model(b_input_ids,  attention_mask = b_masks,labels=b_labels)\n",
    "    return outputs\n",
    "\n",
    "#call model with a batch of input\n",
    "def output_one_batch(batch):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_labels = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    outputs  = model(b_input_ids,  num_beams=2 ,  attention_mask = b_masks,labels=b_labels)\n",
    "    return outputs\n",
    "\n",
    "#do one epoch for training\n",
    "def train_epoch():\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = process_one_batch( batch)\n",
    "        loss = outputs[0]  \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
    "    print(\"avg_train_loss\",avg_train_loss)  \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 training epoch : \",elapsed_time)\n",
    "    return avg_train_loss\n",
    "\n",
    "#do one epoch for eval\n",
    "def eval_epoch():\n",
    "    t0 = time.time()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:            \n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = process_one_batch(batch)\n",
    "            loss = outputs[0]              \n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss         \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"avg_val_loss\",avg_val_loss) \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 eval epoch : \",elapsed_time)\n",
    "    return avg_val_loss\n",
    "    \n",
    "\n",
    "#Runs the model on a set number of batches and saves the results to a json file\n",
    "def save_results( num_batches = 15, iter = 0):\n",
    "    indexes_list = []\n",
    "    inputs_list = []\n",
    "    predicted_list = []\n",
    "    expected_list = []\n",
    "    \n",
    "    model.save_pretrained(\"saved_model_temp\")\n",
    "\n",
    "    #for i in tqdm(range(num_examples)):\n",
    "    for i, batch in enumerate(tqdm(validation_dataloader)):\n",
    "        if(num_batches != None and i>num_batches):\n",
    "            break\n",
    "        # Story is:\n",
    "        #input_ids = tokenizer(val_data.input.iloc[i], return_tensors='pt')\n",
    "        #input_ids.to(device)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        \n",
    "        greedy_output = model.generate(\n",
    "                b_input_ids,  #check stars   \n",
    "                num_beams=1 ,\n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True, \n",
    "                max_length=150,\n",
    "                tempterature = .9,\n",
    "                top_p = .7\n",
    "                )\n",
    "        \n",
    "        print(greedy_output['sequences'].shape)\n",
    "        output = tokenizer.batch_decode(greedy_output['sequences'])\n",
    "        len_input = len(val_data.input.iloc[i])\n",
    "        #output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "        b_outputs = []\n",
    "        \n",
    "        for b_idx in range(BATCH_SIZE):\n",
    "            if(len(output) <= b_idx):\n",
    "                break\n",
    "            idx = BATCH_SIZE * i + b_idx\n",
    "            end_words = val_data.InputSentence4\n",
    "            in_words = val_data.input.iloc[idx]\n",
    "            \n",
    "            indexes_list.append(idx)\n",
    "            inputs_list.append(val_data.input.iloc[idx])\n",
    "            #predicted_list.append(output[b_idx])\n",
    "            if(len(re.split('\\? |! |\\. ',output[b_idx]))<5):\n",
    "                predicted_list.append('bad gen')\n",
    "            else:\n",
    "                predicted_list.append(re.split('\\? |! |\\. ',output[b_idx])[4] + '.')\n",
    "            expected_list.append(val_data.InputSentence5.iloc[idx])\n",
    "            \n",
    "    outputs = pd.DataFrame()\n",
    "    outputs['inputs'] = inputs_list\n",
    "    outputs['predicted'] = predicted_list\n",
    "    outputs['expected'] = expected_list\n",
    "    \n",
    "    print(outputs[:5])\n",
    "    \n",
    "    data = {}\n",
    "    data['ex'] = []\n",
    "    for i in range(len(indexes_list)):\n",
    "        data['ex'].append({\n",
    "            'idx': indexes_list[i],\n",
    "            'input': inputs_list[i],\n",
    "            'prediction': predicted_list[i],\n",
    "            'expected': expected_list[i]\n",
    "\n",
    "        })\n",
    "\n",
    "    with open('test_cases/test_cases'+ str(iter)+'.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for i in range(15):\n",
    "    train_loss.append(train_epoch())\n",
    "    val_loss.append(eval_epoch())\n",
    "    if(i<7):\n",
    "        save_results(1, i)\n",
    "    else:\n",
    "        save_results(1, i)\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.show()\n",
    "    plt.savefig('losses.png')\n",
    "    model.save_pretrained(\"UNION_model\")\n",
    "\n",
    "\n",
    "\n",
    "save_results(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f0e40",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.makedirs(\"saved_model\")\n",
    "#model.save_pretrained(\"saved_model\")\n",
    "#model = .from_pretrained(\"path/to/awesome-name-you-picked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default config\n",
    "#configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "# Load pretrained gpt2\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"saved_model/\", config=configuration)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"saved_model/\")#, config=configuration)\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create device\n",
    "device = torch.device(\"cuda:1\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0000005)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#do one epoch for eval\n",
    "def save_results( num_batches = 15, iter = 0):\n",
    "    indexes_list = []\n",
    "    inputs_list = []\n",
    "    predicted_list = []\n",
    "    expected_list = []\n",
    "        \n",
    "    #for i in tqdm(range(num_examples)):\n",
    "    for i, batch in enumerate(tqdm(validation_dataloader)):\n",
    "        if(num_batches != None and i>num_batches):\n",
    "            break\n",
    "        # Story is:\n",
    "        #input_ids = tokenizer(val_data.input.iloc[i], return_tensors='pt')\n",
    "        #input_ids.to(device)\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        \n",
    "        greedy_output = model.generate(\n",
    "                b_input_ids,  #check stars   \n",
    "                num_beams=2 ,\n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True, \n",
    "                max_length=150,\n",
    "                #tempterature = 5,\n",
    "                top_p = 10\n",
    "                )\n",
    "        \n",
    "        print(greedy_output['sequences'].shape)\n",
    "        output = tokenizer.batch_decode(greedy_output['sequences'])\n",
    "        len_input = len(val_data.input.iloc[i])\n",
    "        #output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "        b_outputs = []\n",
    "        \n",
    "        for b_idx in range(BATCH_SIZE):\n",
    "            if(len(output) <= b_idx):\n",
    "                break\n",
    "            idx = BATCH_SIZE * i + b_idx\n",
    "            end_words = val_data.InputSentence4\n",
    "            in_words = val_data.input.iloc[idx]\n",
    "            #if(len(output) <= b_idx):\n",
    "            #    break\n",
    "            #b_out = output[b_idx]\n",
    "            #pred_sent = b_out[b_out.index('<sos>') + len(in_words) + 13:]\n",
    "            #if(pred_sent.find('.') != -1):\n",
    "            #    pred_sent = pred_sent[:pred_sent.index('.')+1]\n",
    "            #else:\n",
    "            #    pred_sent = pred_sent\n",
    "            \n",
    "            indexes_list.append(idx)\n",
    "            inputs_list.append(val_data.input.iloc[idx])\n",
    "            #predicted_list.append(output[b_idx])\n",
    "            if(len(re.split('\\? |! |\\. ',output[b_idx]))<5):\n",
    "                predicted_list.append('bad gen')\n",
    "            else:\n",
    "                predicted_list.append(re.split('\\? |! |\\. ',output[b_idx])[4] + '.')\n",
    "            expected_list.append(val_data.InputSentence5.iloc[idx])\n",
    "            \n",
    "    outputs = pd.DataFrame()\n",
    "    outputs['inputs'] = inputs_list\n",
    "    outputs['predicted'] = predicted_list\n",
    "    outputs['expected'] = expected_list\n",
    "    \n",
    "    print(outputs[:5])\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    data = {}\n",
    "    data['ex'] = []\n",
    "    for i in range(len(indexes_list)):\n",
    "        data['ex'].append({\n",
    "            'idx': indexes_list[i],\n",
    "            'input': inputs_list[i],\n",
    "            'prediction': predicted_list[i],\n",
    "            'expected': expected_list[i]\n",
    "\n",
    "        })\n",
    "\n",
    "    with open('test_cases/test_cases'+ str(iter)+'.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "save_results(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8f91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt23",
   "language": "python",
   "name": "gpt23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
